# Task Statement 2.1: Explain the Basic Concepts of Generative AI

## Overview of Generative AI

- **Generative AI** is a subset of deep learning focused on generating new, original content rather than classifying or predicting existing content.
- It is capable of creating text, images, audio, video, and code.
- Generative AI models learn patterns and representations from large datasets and use this knowledge to generate outputs that resemble the training data.

## Foundation Models

- Generative AI utilizes **foundation models** trained on vast amounts of data.
- These models identify statistical patterns in various modalities, such as natural language and images.
- Foundation models are large neural networks with billions of parameters, enabling advanced tasks.
- Models can be used as-is or fine-tuned for specific use cases.

## Core Components of Generative AI

- **Models**: Built with neural networks, system resources, data, and prompts.
- **Training**: Models are trained to generate unique outputs based on learned knowledge.
- **Inference**: The process where the model generates an output (completion) based on input data (prompt).

## Transformer Networks

- The core architecture for generative AI is the **transformer network**.
- Introduced in the 2017 paper "Attention Is All You Need."
- Large Language Models (LLMs), such as ChatGPT, are based on transformer architecture.
- LLMs are pre-trained on massive text datasets and can be fine-tuned for specific tasks.

## Key Concepts

- **Prompt**: The input provided to the model to generate an output.
- **Inference**: The process of generating an output from a prompt.
- **Completion**: The output generated by the model in response to a prompt.
- **Context Window**: The amount of input data (tokens) the model can consider at once.
- **Tokens**: Units of text (words or subwords) processed by the model.
- **Vocabulary**: The set of tokens the model can understand.
- **Tokenizer**: The tool that converts raw text into tokens.
- **Prompt Engineering**: The practice of designing prompts to guide the model toward desired outputs.

## Statistical Foundations

- Generative AI models rely on statistics and linear algebra, including:
  - Probability modeling
  - Loss functions
  - Matrix multiplication

## Prompt Engineering and In-Context Learning

- **Prompt Engineering**: Crafting prompts to achieve better model outputs.
- **In-Context Learning**: Providing examples within the prompt to help the model understand the task.
  - **Few-shot**: Multiple examples provided.
  - **One-shot**: A single example provided.
  - **Zero-shot**: No examples provided.

## Inference Configuration

- The prompt, along with inference configuration parameters, influences the model's output.
- Prompts can include instructions, content, and in-context examples to improve completions.

---